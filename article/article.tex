%- Efter "..constant query time and linear space" skal der være referencer.
%  * Til hvem? Navarri refererer til konstanttids NCA og RMQ men ikke til en kilde der beskriver hvordan de bruges til at lave LCE. Johannes Fischer skriver bare "It is well-known".
%- Er navnene på algoritmerne de samme som i de oprindelige artikler? I
%  så fald, så drop "we call". Der skal selvfølgelig også være
%  referencer i dette afsnit lige i starten af definitionen på hver
%  algoritme/datastruktur. Skriv gerne navnene på forfatterne: "Navarro
%  et al[?] gave an algorithm..." eller lign. 
%  * DirectComp har jeg ikke selv navngivet. SuffixNca og LcpRmq navnene har jeg selv fundet på. De har ikke noget navn i de tidligere artikler.

% hav plot for $r=\sqrt n$ med til forsvar

\documentclass[a4]{article}
\usepackage[utf8]{inputenc}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{clrscode}
\usepackage[draft]{fixme}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{textcomp}
\usepackage{enumerate}

\usepackage{prettyref}
\newcommand{\sectionname}{Section}
% \renewcommand{\figurename}{Figure}
\newcommand*{\pref}{\prettyref}
% Adds varioref to the prettyrefs.
% Replacing vref with ref to avoid page numbers
\newrefformat{cha}{\chaptername~\ref{#1}}
\newrefformat{sec}{\sectionname~\ref{#1}}
\newrefformat{fig}{\figurename~\ref{#1}}
\newrefformat{eq}{\equationname~\ref{#1}}
\newrefformat{tab}{\tablename~\ref{#1}}
\newrefformat{app}{\appendixname~\ref{#1}}
\newrefformat{alg}{Algorithm~\ref{#1}}

\title{Algorithms for Longest Common Extensions} 

\author{Jesper Kristensen\\DTU Informatics\\Technical University of Denmark}

\date{August 1, 2011}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}

% http://texblog.net/latex/layout/page/2/
\fontdimen2\font=0.25em% interword space
\fontdimen3\font=1em% interword stretch
\fontdimen4\font=0.10em% interword shrink

\begin{document}

\newif\ifarticle
\newif\ifreport

\articletrue

\maketitle

\newcommand{\sortt}{\textit{sort}(n,\sigma)}
\newcommand{\LCE}{\textit{LCE}}
\newcommand{\NCA}{\textit{NCA}}
\newcommand{\RMQ}{\textit{RMQ}}
\newcommand{\SA}{\textit{SA}}
\newcommand{\SAinv}{\textit{SA}^{-1}} % math
\newcommand{\SAi}{SA$^{-1}$} % no math
\newcommand{\LCP}{\textit{LCP}}
\newcommand{\LA}{\textit{LA}}
\newcommand{\suff}{\textit{suff}}
\newcommand{\logceil}{\lceil\log n\rceil}
\newcommand{\fprint}[1][k]{\ensuremath{\proc{Fingerprint}_{#1}}}
\newcommand{\fprintk}{\fprint[k]}
\newcommand{\RMQpq}[2]{RMQ\textless$#1$, $#2$\textgreater}
\newcommand{\RMQn}{\RMQpq{1}{n}}
\newcommand{\RMQq}{\RMQpq{n}{1}}
\newcommand{\RMQlog}{\RMQpq{n}{\log n}}

\hyphenation{Di-rect-Comp Di-rect-Look-up Di-rect-Comp-Look-up Fin-ger-print}

%\tableofcontents

%\vspace{1cm}
% stuff on toc page

%\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction\label{sec:intro}}

The \emph{longest common extension} (LCE) problem is to preprocess a string in order to allow for a large number of LCE querie, such that the queries are efficient. The LCE value, $\LCE_s(i,j)$, is the length of the longest common prefix of the pair of suffixes starting at index $i$ and $j$ in the string $s$. The LCE problem can be used in many algorithms for solving other algorithmic problems, e.g.\ the Landau-Vishkin algorithm for approximate string searching~\cite{approx-search}. Solutions with linear space with constant query time exists for the problem~[TODO cite who]. These theoretically good solutions are however not the best in practice, since they have large constant factors for both time and space usage. In average cases, a much simpler solution with worst case linear time, average case constant time, and no preprocessing has significantly better practical performance~\cite{ilie-navarro-tinta}. This algorithm is ideal when only average case performance is relevant. In situations where we need both average case and worst case performance to be good, none of the existing solutions are ideal. Such a situation could be use of approximate string searching in a firewall, which should not allow an attacker to significantly degrade its performance by sending it carefully crafted packages, while the average case of scanning legitimate data must still be highly performant.

In this paper we give a new algorithm, which is a generalization of the simple linear time algorithm, which is almost as simple, yet it has significantly better worst case query times. This algorithm also achieves significantly better average case practical query times compared to the theoretically best algorithms. The algorithm uses string fingerprinting to achieve a time/space-tradeoff with $O(k\cdot n^{1/k})$ worst case query time, constant average case query time, and $O(kn)$ space for $k$ between one and $\log n$.

\ifreport

In \pref{sec:algorithms} we theoretically describe and analyze the LCE algorithms introduced here in \pref{sec:intro}, and in \pref{sec:results} we analyze results from practical C++ implementations of these algorithms.

We also look at how to apply the LCE problem to Ziv-Lempel compressed strings, such that we can answer LCE queries fast while only using space relative to the compressed size of the string. In \pref{sec:lz-compress} we describe LZ compression, and how we can convert the simple linear time LCE algorithm to work on compressed strings. It uses space linear to the size of the compressed string, and it has worst case query time linear to the size of the uncompressed string, and average case query time $O(\log\log N)$. In \pref{sec:tree-lce} we look at how we can use the tree structure of LZ compression to get better query times on compressed strings.

\fi % report

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{The LCE Problem}

%Given a string $s$ of length $n$ and a pair of indexes $1\leq i\leq n$ and $1\leq j\leq n$, the \emph{longest common extension} of $i$ and $j$ on $s$, $\LCE_s(i,j)$, is the length of the longest common prefix of $\suff_i$ and $\suff_j$. If for example $s = abbababba$, $i = 4$ and $j=6$, then $\suff_4 = ababba$ and $\suff_6 = abba$. The longest common prefix of these two suffixes is $ab$, which has length $2$, and therefore we have $\LCE_s(4,6) = 2$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Previous Results\label{sec:existing-results}}

Ilie~et~al.~\cite{ilie-navarro-tinta} gave and algorithm, \proc{DirectComp}, for solving the LCE problem, which uses no preprocessing and $O(|\LCE(i,j)|)$ query time. For a query $\LCE(i,j)$, the algorithm compares $s[i]$ to $s[j]$, then $s[i+1]$ to $s[j+1]$ and so on, until the two characters differ, or the end of the string is reached. The worst case query time is $O(n)$ on a string of length $n$.
Given a string length $n$ and an alphabet size $\sigma$, we define \emph{average case query time} as the average of query times over all $\sigma^n\cdot n^2$ combinations of strings and query inputs.
The average case LCE value over all possible inputs of a given string of length $n$ and alphabet size $\sigma$ is $O(1/(\sigma-1))=O(1)$~\cite{ilie-navarro-tinta}, which gives \proc{DirectComp} average case query time $O(1)$.
The LCE problem can be solved with $O(1)$ worst case query time, using $O(n)$ space and $O(\sortt)$ preprocessing time. Two different ways of doing this exists.
One method, \proc{SuffixNca}, uses constant time nearest common ancestor (NCA) queries on a suffix tree. The LCE of two indexes $i$ and $j$ is defined as the length of the longest common prefix of $\suff_i$ and $\suff_j$. In a suffix tree, the path from the root to $L_i$ has label $\suff_i$ (likewise for $j$), and no two child edge labels of the same node will have the same first character. The longest common prefix of the two suffixes will therefore be the path label from the root to the nearest common ancestor of $L_i$ and $L_j$. I.e. $\LCE_s(i,j) = D[\NCA_\mathcal{T}(L_i,L_j)]$.
The other method, \proc{LcpRmq}, uses constant time range minimum queries (RMQ) on a longest common prefix (LCP) array. The LCP array contains the length of the longest common prefixes of each pair of neighbor suffixes in the suffix array (SA). The length of the longest common prefix of two arbitrary suffixes in SA can be found as the minimum of all LCP values of neighbor suffixes between the two desired suffixes, because SA lists the suffixes in lexicographical ordering. I.e. $\LCE(i,j)=\LCP[\RMQ_{\LCP}(\SAinv[i] + 1, \SAinv[j])]$, where $\SAinv[i] < \SAinv[j]$.
Ilie~et~al.~\cite{ilie-navarro-tinta} have looked at a number of real world texts as well as texts of randomly generated characters, and found that all the texts they examined each has an average LCE of at most one, over all $n^2$ input pairs. Therefore it is interesting to have LCE algorithms, which perform good on average when the LCE value is small.
Both \proc{SuffixNca} and \proc{LcpRmq} have the same asymptotic space and times. In practice, \proc{LcpRmq} is the best of the two~\cite{ilie-navarro-tinta}. The constant factor for average case query time of \proc{DirectComp} is much smaller than the constant factor for query time of \proc{LcpRmq}, thus \proc{DirectComp} is better in practice on average case inputs than the theoretically best \proc{LcpRmq}.

\ifreport
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Our Fingerprinting Algorithm}
\fi % report
\ifarticle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Our Results}
\fi % article

We present a new LCE algorithm, \fprintk, where $k$ is a parameter $1\leq k\leq\logceil$, which describes the number of levels used\footnote{All logarithms are base two.}. This algorithm is based on string fingerprinting.

\begin{theorem}
For a string $s$ of length $n$ and alphabet size $\sigma$, the \fprintk\ algorithm, where $k$ is a parameter $1 \leq k \leq \logceil$, can solve the LCE problem in $O(k\cdot n^{1/k})$ worst case query time and $O(1)$ average case query time using $O(k\cdot n)$ space and $O(\sortt + k\cdot n)$ preprocessing time.
\end{theorem}

The exact worst case performance of our solution depends on the amount of space you are willing to use.

\begin{corollary}
\fprint[1] is equivalent to \proc{DirectComp} with $O(n)$ space and $O(n)$ query time.
\end{corollary}

\begin{corollary}
\fprint[2] uses $O(n)$ space and $O(\sqrt n)$ query time. It has two levels, where one uses a table of fingerprints and the other uses the original string.
\end{corollary}

\begin{corollary}
\fprint[\logceil] uses $O(n\cdot\log n)$ space and $O(\log n)$ query time. The data structure is equivalent to the one generated by Karp-Miller-Rosenberg~\cite{karp-miller-rosenberg}, and a query only needs to do one comparison at each level.
\end{corollary}

To preprocess the $O(k\cdot n)$ fingerprints used by our algorithm, we can use Karp-Miller-Rosenberg~\cite{karp-miller-rosenberg}, which takes $O(n\log n)$ time. For $k=o(\log n)$, we can speed up preprocessing to $O(\sortt + k\cdot n)$ by using the SA and LCP arrays.

\pref{tab:article-algorithms} shows an overview of asymptotic bounds of the different LCE algorithms.

\begin{table}[tp]
\centering
% \begin{tabular}{l|p{160px}|p{160px}}
\begin{tabular}{l|l|l|l}
\hline\hline
Algorithm & Space & Query time & Preprocessing \\ [0.5ex] \hline
\proc{SuffixNca} & $O(n)$ & $O(1)$ & $O(\sortt)$ \\ \hline
\proc{LcpRmq} & $O(n)$ & $O(1)$ & $O(\sortt)$ \\ \hline
\proc{DirectComp} & $O(1)$ & $O(n)$ & $O(1)$ \\ \hline
\fprintk * & $O(k\cdot n)$ & $O(k\cdot n^{1/k})$ & $O(\sortt+k\cdot n)$ \\
~~~~$k=\logceil$ * & $O(n\log n)$ & $O(\log n)$ & $O(n\log n)$ \\ \hline
\end{tabular}
\caption{LCE algorithms with their space requirements, worst case query times and preprocessing times. Average case query times are $O(1)$ for all shown algorithms. Rows marked with * show the new algorithm we present.}\label{tab:article-algorithms}
\end{table}

%\begin{figure}[tp]
%    \begin{center}
%        \includegraphics[width=0.5\textwidth,page=1]{wc-avg.pdf}
%    \end{center}
%    \caption{\label{fig:wc-avg}Graphical comparison of query times for different LCE algorithms.}
%\end{figure}

In practice, existing state of the art solutions are either good in worst case, while poor in average case (\proc{LcpRmq}), or good in average case while poor in worst case (\proc{DirectComp}). Our \fprintk\ solution targets a worst case vs. average case query time tradeoff between these two extremes. Our solution is almost as fast as \proc{DirectComp} on an average case input, and it is significantly faster than \proc{DirectComp} on a worst case input. Compared to \proc{LcpRmq}, our solution has a significantly better performance on an average case input, but its worst case performance is not as good as that of \proc{LcpRmq}. The space usage for \proc{LcpRmq} and \fprintk\ are approximately the same when $k=6$.

Our algorithm is fairly simple. Though it is slightly more complicated than \proc{DirectComp}, it does not use any of the advanced algorithmic techniques required by \proc{LcpRmq} and \proc{SuffixNca}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Overview}
In \pref{sec:fprintk} we present the \fprintk\ algorithm and analyze its theoretical complexity, and in \pref{sec:results} we present our findings from tests of a practical implementation of \fprintk, which we compare against the existing algorithms.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Preliminaries}

\paragraph{String notation.} Let $s$ be a string of length $n$. Then $s[i]$ is the $i$'th character of $s$, and $s[i\twodots j]$ is a substring of $s$ containing characters $s[i]$ to $s[j]$, both inclusive. That is, $s[1]$ is the first character of $s$, $s[n]$ is the last character, and $s[1\twodots n]$ is the entire string. The suffix of $s$ starting at index $i$ is written $\suff_i = s[i\twodots n]$.

\ifreport
\paragraph{Tree notation.}
The ancestors of a node $u$ is $u$ itself as well as any node, which is a parent of an ancestor of $u$. A path in a tree begins at node $u$ and ends at node $v$, where $u$ is an ancestor of $v$. The length of a path is the number of edges between nodes in the path. The depth of a node $u$ is the length of the path from the root to $u$, and the height of $u$ is the length of the longest path from $u$ to a descendant of $u$. E.g. the depth of the root and the height of a leaf is both zero.
\fi % report

\paragraph{Sorting complexity.} The time it takes to sort $n$ numbers within an universe of size $\sigma$ is written as $\sortt$.

\paragraph{Suffix tree.} A suffix tree $\mathcal{T}$ encodes all suffixes of a string $s$ of length $n$ with alphabet $\sigma$. The tree has $n$ leaves named $L_1$ to $L_n$, one for each suffix of $s$. Each edge is labeled with a substring of $s$, such that for any $1\leq i\leq n$, the concatenation of labels on edges on the path from the root to $L_i$ gives $\suff_i$. Any internal node must have more than one child, and the labels of two child edges must not share the same first character. The string depth $D[v]$ of a node $v$ is the length of the string formed when concatenating the edge labels on the path from the root to $v$.
The tree uses $O(n)$ space, and building it takes $O(\sortt)$ time~\cite{sort-complexity}.

\ifreport

There are $n$ leaves in a suffix tree, and since each internal node has at least two children, the tree has less than $2n$ nodes. Each edge uses $O(1)$ space, since the label is a substring of $s$ and it can therefore be represented by two indexes of $s$. The suffix tree therefore takes $O(n)$ space.

\fi % report

\paragraph{SA and LCP.} For a string $s$ of length $n$ with alphabet size $\sigma$, the \emph{suffix array} (\SA) is an array of length $n$, which encodes the lexicographical ordering of all suffixes of $s$. The lexicographically smallest suffix is $\suff_{\SA[1]}$, the lexicographically largest suffix is $\suff_{\SA[n]}$, and the lexicographically $i$'th smallest suffix is $\suff_{\SA[i]}$. The \emph{inverse suffix array} (\SAi) describes where a given suffix is in the lexicographical order. Suffix $\suff_i$ is the lexicographically $\SAinv[i]$'th smallest suffix.

The \emph{longest common prefix array} (LCP array) describes the length of longest common prefixes of neighboring suffixes in SA. The length of the longest common prefix of $\suff_{\SA[i-1]}$ and $\suff_{\SA[i]}$ is $\LCP[i]$, for $2 \leq i \leq n$. The first element $\LCP[1]$ is always zero.

Building the SA, \SAi\ and LCP arrays takes $O(\sortt)$ time~\cite{sort-complexity}.

\paragraph{NCA.} The ancestors of a node $u$ is $u$ itself as well as any node, which is a parent of an ancestor of $u$. The \emph{nearest common ancestor} (NCA) of two nodes $u$ and $v$ is the node of greatest depth, which is an ancestor of both $u$ and $v$. An NCA query can be answered in $O(1)$ time with $O(n)$ space and preprocessing time in a static tree with $n$ nodes~\cite{nca}.

\paragraph{RMQ.} The range minimum of $i$ and $j$ on an array $A$ is the index of a minimum element in $A[i,j]$, i.e.\ $\RMQ_A(i,j) = \arg \min_{k\in\{i,...,j\}}\{A[k]\}$. A \emph{range minimum query} (RMQ) on a static array of $n$ elements can be answered in $O(1)$ time with $O(n)$ space and preprocessing time~\cite{jf-rmq}.

\paragraph{I/O and Cache-Oblivious Model.}
The I/O model describes the number of memory blocks an algorithm moves between two layers of a layered memory architecture, where the size of the internal memory layer is $M$ words, and data is moved between internal and external memory in blocks of $B$ words. In the \emph{cache-oblivious model}, the algorithm has no knowledge of the values of $M$ and $B$.

\ifreport

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{LCE Algorithms\label{sec:algorithms}}
This section describes existing and new algorithms for solving the LCE problem. It contains a theoretical analysis of their asymptotic query times and space requirements, shown in summary in \pref{tab:algorithms}. \pref{sec:results} describes the results from experiments on practical implementations of these algorithms.

\begin{table}[tp]
\centering
% \begin{tabular}{l|p{160px}|p{160px}}
\begin{tabular}{l|l|l|l}
\hline\hline
Algorithm & Space & Query time & Preprocessing \\ [0.5ex] \hline
\proc{SuffixNca} & $O(n)$ & $O(1)$ & $O(\sortt)$ \\ \hline
\proc{LcpRmq} & $O(n)$ & $O(1)$ & $O(\sortt)$ \\ \hline
\proc{DirectComp} & $O(1)$ & $O(n)$ & $O(1)$ \\ \hline
\proc{DirectCompLookup}, $1 \leq t \leq n$ & $O(n^2/t)$ & $O(t)$ & $O(n^2)$ \\
$t=1$ & $O(n^2)$ & $O(1)$ & $O(n^2)$ \\ \hline
\fprintk, $1 \leq k \leq \logceil$ & $O(k\cdot n)$ & $O(k\cdot n^{1/k})$ & $O(\sortt+k\cdot n)$ \\
$k=\logceil$ & $O(n\log n)$ & $O(\log n)$ & $O(n\log n)$ \\ \hline
\end{tabular}
\caption{LCE algorithms with their space requirements, worst case query times and preprocessing times. Average case query times are $O(1)$ for all shown algorithms.}\label{tab:algorithms}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Existing Algorithms}

In this section we look at the existing LCE algorithms described in \pref{sec:existing-results} with a bit more details.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{\proc{DirectComp}}

\proc{DirectComp} solves the LCE problem without any preprocessing in $O(|\LCE(i,j)|)$ query time. For a query $\LCE(i,j)$, the algorithm compares $s[i+v]$ to $s[j+v]$ in a loop, for $v$ starting at zero and incrementing, until the two characters differ, or the end of the string is reached, at which point the LCE value is $v$. \proc{DirectComp} does not need to check if the end of the string is reached, i.e. $i+v>n$ or $j+v>n$. Instead we can preprocess the string by adding a special character \$ to the end of the string, which does not occur anywhere else in the string. When the algorithm compares $s[n] = \$$ to any other character, it will stop, as the characters differ. This can give better practical performance compared to checking $i+v>n$ and $j+v>n$ in each iteration of the algorithm. The query time is $O(|\LCE(i,j)|)$, which in worst case is $O(n)$ on a string of length $n$, and in average case is $O(1)$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{\proc{SuffixNca}: NCA on a Suffix Tree}

LCE of two indexes $i$ and $j$ is defined as the length of the longest common prefix of $\suff_i$ and $\suff_j$. In a suffix tree, the path from the root to $L_i$ has label $\suff_i$ (likewise for $j$). The path label on the path from the root to the nearest common ancestor of $L_i$ and $L_j$ is therefore a common prefix of the two suffixes. Since two child edges of the same node never have the same first character on their edge labels, the path label on the path from the root to $\NCA_\mathcal{T}(L_i,L_j)$ is not only a common prefix, but the longest common prefix of $\suff_i$ and $\suff_j$. We can therefore find the LCE value as $\LCE_s(i,j) = D[\NCA_\mathcal{T}(L_i,L_j)]$, where $\mathcal{T}$ is the suffix tree for $s$, and $D[v]$ is the label depth of node $v$ in the tree.

For a string of length $n$ with alphabet size $\sigma$, we can preprocess \proc{SuffixNca} in $O(\sortt)$ time by building the suffix tree, preprocessing it for NCA and building the array of label depths. The query time is $O(1)$, since looking up $L_i$ and $L_j$, doing an NCA query, and looking up the label depth in $D$ all takes constant time.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{\proc{LcpRmq}: RMQs on a LCP Array}

The LCP array contains the length of the longest common prefixes of each pair of neighbor suffixes in SA. The length of the longest common prefix of two arbitrary suffixes in SA can be found as the minimum of all LCP values of neighbor suffixes between the two desired suffixes, because SA lists the suffixes in lexicographical ordering. I.e. $\LCE(i,j)=\LCP[\RMQ_{\LCP}(\SAinv[i] + 1, \SAinv[j])]$, where $\SAinv[i] < \SAinv[j]$. An example is shown in \pref{fig:sa+lcp+min}. Preprocessing time is $O(\sortt)$, space requirement is $O(n)$ and query time is $O(1)$.

\begin{figure}[tp]
    \begin{center}
        \includegraphics[width=0.2\textwidth,page=1]{sa+lcp+min.pdf}
    \end{center}
    \caption{\label{fig:sa+lcp+min}For $s=abbababba$, $\LCE(2,3)$ is the longest common prefix of $bababba$ and $bbababba$. There are two other suffixes between these in the lexicographical ordering, and the longest common prefixes between each pair of neighbors are 3, 1 and 3. Therefore $\LCE(2,3)= \min ~ \{3, 1, 3\} = 1$.}
\end{figure}

Different algorithms exists for solving the RMQ problem, and we can solve it in $O(1)$ time and $O(n)$ space. But like for the LCE problem, the asymp-\\totically best RMQ solution may not be the best RMQ solution in practice. We examine the following RMQ solutions, where \RMQpq{p}{q} has $O(p)$ space and preprocessing time, and $O(q)$ query time:
\begin{samepage}
\begin{description}
\item[\RMQn] walks through the array directly in $O(n)$ time and does not use any preprocessing (except for transforming LCE into RMQ).
\item[\RMQq] preprocesses the array in $O(n)$ space in order to do queries in $O(1)$ time. This is a twolevel data structure as shown in \pref{fig:sa+lcp+min-twolevel}. The array is split into blocks of size $O(\log n)$. The top level is an array of length $O(n/\log n)$, where each element contains the minimum value of one block. The top level uses a \RMQpq{n\log n}{1}-solution, and the lower level precomputes all answers for each block, which is done in $O(n)$ space, since there are only a limited number of different block types of size $O(\log n)$.
\item[\RMQlog] is the same as \RMQq\ except that the lower level is replaced with a direct search through a $O(\log n)$ length block.
\end{description}
\end{samepage}

\begin{figure}[tp]
    \begin{center}
        \includegraphics[width=0.8\textwidth,page=2]{sa+lcp+min.pdf}
    \end{center}
    \caption{\label{fig:sa+lcp+min-twolevel}Twolevel Range Minimum Query. The range minimum is calculated as the minimum of one query in the top level and two queries in the lower level.}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Combining \proc{DirectComp} and \proc{LcpRmq}\label{sec:dc-lcprmq}}

Ilie~et~al.~\cite{ilie-navarro-tinta} suggests combining \proc{DirectComp} and \RMQn, by using \RMQn\ whenever the two suffixes we want to compare are close to each other in SA, and otherwise use \proc{DirectComp}. However that still gives $O(n)$ worst case query time. For example in the string $s=aaaaa...$, the query $\LCE(1,n/2)$ would require $\Omega(n)$ time for both \proc{DirectComp} and \RMQn.

We can instead combine \proc{DirectComp} and \RMQq, which we call \proc{DirectComp+LcpRmq}. \proc{DirectComp+LcpRmq} first uses \proc{DirectComp} until a specific cutoff and then switches over to \RMQq\ instead. This could have both the good average case query time of \proc{DirectComp} and the good worst case query time of \proc{LcpRmq}. However it still keeps the large space requirements of \proc{LcpRmq}, and it is not simple to implement like \proc{DirectComp}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The \proc{DirectCompLookup} Algorithm}

The input to the $LCE$ problem is two numbers $1 \leq i < j \leq n$ \footnote{We can assume that $i< j$ without loss of generality, since $\LCE(i,j)=\LCE(j,i)$}, so there are $n\cdot(n-1)/2 = O(n^2)$ different input possibilities in total. Because of this limited space of possible query inputs, we can tabulate a number of LCE values, and use those to speed up queries.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{\proc{DirectLookup}}

We can store results for all $O(n^2)$ possible queries in $O(n^2)$ space, and a query will then be a single table lookup in $O(1)$ time. We call this algorithm \proc{DirectLookup}. A result space table for \proc{DirectLookup} is shown in \pref{fig:resultspace-empty}, and \pref{fig:resultspace-directcomp} shows how \proc{DirectComp} moves through that table.

We can preprocess the table in in $O(n^2)$ time, since each value can be calculated in constant time like follows, if $LCE(i+1,j+1)$ is always calculated before $LCE(i,j)$ for any $i$ and $j$:
\begin{align*}
LCE(i,j) &=
\begin{cases}
    0 & \textrm{if} ~ s[i] \neq s[j] \\
    1 & \textrm{if} ~ s[i] = s[j] \wedge (i = n \lor j = n) \\
    LCE(i+1, j+1) + 1 & \textrm{otherwise} \\
\end{cases}
\end{align*}

\begin{figure}[p]
    \begin{center}
        \includegraphics[width=0.42\textwidth,page=1]{resultspace.pdf}
    \end{center}
    \caption{\label{fig:resultspace-empty}The result space for $LCE_s(i,j)$ for $1 \leq i < j \leq n$ where $s = abacbbacbacacabc$.}
\end{figure}

\begin{figure}[p]
    \begin{center}
        \includegraphics[width=0.42\textwidth,page=2]{resultspace.pdf}
    \end{center}
    \caption{\label{fig:resultspace-directcomp}$LCE(3,7)$ using \proc{DirectComp}.}
\end{figure}

\begin{figure}[p]
    \begin{center}
        \includegraphics[width=0.42\textwidth,page=3]{resultspace.pdf}
    \end{center}
    \caption{\label{fig:resultspace-t-row}$LCE(3,7)$ with distance $t=4$ between each stored row.}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{\proc{DirectComp} and \proc{DirectLookup} Combined}

\proc{DirectComp} and \proc{DirectLookup} preprocesses and stores no results and all results respectively. If we only store some results, we can use \proc{DirectComp} until we reach a stored result, and then retrieve this result using \proc{DirectLookup}. If we store every $t$'th row, as shown in \pref{fig:resultspace-t-row}, the space usage will be $O(n^2/t)$ and the worst case query time will be $O(t)$. We call this algorithm \proc{DirectCompLookup}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The \fprintk\ Algorithm}

\fi % report

\ifarticle
\section{The \fprintk\ Algorithm\label{sec:fprintk}}
\fi % article

Our \fprintk\ algorithm generalizes \proc{DirectComp}. It compares characters starting at positions $i$ and $j$, but instead of comparing individual characters, it compares fingerprints of substrings. Given fingerprints of all substrings of length $t$, our algorithm can compare two $t$-length substrings in constant time.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Data Structure\label{sec:fingerprint-ds}}

Given a string $s$, the fingerprint $F_t[i]$ is a natural number identifying the substring $s[i\twodots i+t-1]$ among all $t$-length substrings of $s$. We assign fingerprints such that for any $i$, $j$ and $t$, $F_t[i] = F_t[j]$ if and only if $s[i\twodots i+t-1] = s[j\twodots j+t-1]$. In other words, if two substrings of $s$ have the same length, they have the same fingerprints if and only if the substrings themselves are the same.

At the end of a string when $i+t-1>n$, we define $F_t[i]$ by adding extra characters to the end of the string as needed.

The \fprintk\ data structure for a string $s$ of length $n$, where $k$ is a parameter $1 \leq k \leq \logceil$, consists of $k$ natural numbers $t_0$, ..., $t_{k-1}$ and $k$ tables $H_0$, ..., $H_{k-1}$, each of length $n$. For each $\ell$ where $0\leq \ell\leq k-1$, $t_\ell = \Theta(n^{\ell/k})$ and table $H_\ell$ contains fingerprints of all $t_\ell$-length substrings of $s$, such that $H_\ell[i] = F_{t_\ell}[i]$. We always have $t_0 = n^{0/k} = 1$, such that $H_0$ is the original string $s$. The last character of the string must be a special character \$, which does not occur anywhere else in the string. An example is shown in \pref{fig:fingerprint-ds}.

\begin{figure}[tp]
    \begin{center}
        \includegraphics[width=0.98\textwidth,page=1]{../doc/fingerprint.pdf}
    \end{center}
    \caption{\label{fig:fingerprint-ds}\fprintk\ data structure for $s=\textit{abbaabbababbaabbababaababa\$}$, $n=27$, $k=3$, $t_1=n^{1/3}=3$ and $t_2=n^{2/3}=9$. All substrings $\textit{bba}$ are highlighted with their $3$-length fingerprint $2$.}
\end{figure}

\begin{lemma}
The \fprintk\ data structure takes $O(k\cdot n)$ space.
\end{lemma}
\begin{proof}
Each of the $k$ tables stores $n$ fingerprints of constant size.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Query\label{sec:fingerprint-query}}

To perform a LCE query using the \fprintk\ data structure, start with $v=0$ and $\ell=0$, then do the following steps:
\begin{samepage}
\begin{enumerate}
\item As long as $H_\ell[i+v] = H_\ell[j+v]$, increment $v$ by $t_\ell$, increment $\ell$ by one, and repeat this step unless and $\ell = k-1$.
\item As long as $H_\ell[i+v] = H_\ell[j+v]$, increment $v$ by $t_\ell$ and repeat this step.
\item Stop and return $v$ when $\ell = 0$, otherwise decrement $\ell$ by one and go to step two.
\end{enumerate}

An example of a query is shown in \pref{fig:fingerprint-query}.
\end{samepage}

\begin{figure}[tp]
    \begin{center}
        \includegraphics[width=0.98\textwidth,page=2]{../doc/fingerprint.pdf}
    \end{center}
    \caption{\label{fig:fingerprint-query}\fprintk\ query for $\LCE(3,12)$ on the data structure of \pref{fig:fingerprint-ds}. The top half shows how $H_\ell[i+v]$ moves through the data structure, and the bottom half shows $H_\ell[j+v]$.}
\end{figure}

\begin{lemma}
The \fprintk\ query algorithm is correct.
\end{lemma}
\begin{proof}
At each step of the algorithm $v \leq \LCE(i,j)$, since the algorithm only increments $v$ by $t_\ell$ when it has found two matching fingerprints, and fingerprints of two substrings of the same length are only equal if the substrings themselves are equal. When the algorithm stops, it has found two fingerprints, which are not equal, and the length of these substrings is $t_\ell = 1$, therefore $v = \LCE(i,j)$.

The algorithm never reads $H_\ell[x]$, where $x>n$, because the string contains a unique character \$ at the end. This character will be at different positions in the substrings whose fingerprints are the last $t_\ell$ elements of $H_\ell$. These $t_\ell$ fingerprints will therefore be unique, and the algorithm will not continue at level $\ell$ after reading one of them.
\end{proof}

\begin{lemma}
The worst case query time for \fprintk\ is $O(k\cdot n^{1/k})$, and the average case query time is $O(1)$.
\end{lemma}
\begin{proof}
Step one takes $O(k)$ time. In step two and three, the number of remaining characters left to check at level $\ell$ is $O(n^{(\ell+1)/k})$, since the previous level found two differing substrings of that length (at the top level $\ell=k-1$ we have $O(n^{(\ell+1)/k}) = O(n)$). Since we can check $t_\ell = \Theta(n^{\ell/k})$ characters in constant time at level $\ell$, the algorithm uses $O(n^{(\ell+1)/k})/\Theta(n^{\ell/k}) = O(n^{1/k})$ time at that level. Over all $k$ levels, $O(k\cdot n^{1/k})$ query time is used.

At each step except step three, the algorithm increments $v$. Step three is executed the same number of times as step one, in which $v$ is incremented. The query time is therefore linear to the number of times $v$ is incremented, and it is thereby $O(v)$. The query time is thus $O(1)$ in average case, where $v=O(1)$.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Average Case Optimization\label{sec:improved-avg}}

We could have left out step one of the query algorithm and started with $\ell=k-1$. This would keep the asymptotic worst case query time of $O(k\cdot n^{1/k})$, while it might improve practical worst case query time, but it would increase our average case query time to $O(k)$.
\ifarticle
Our experiments have shown that whenever $k$ is small, keeping step one improves average case query time, while it does not have a measurable effect on worst case query times. When $k = \logceil$, keeping step one can double the worst case query time, while it can make the average case query time 40 times faster for an input string of ten million characters. We want to optimize our LCE query time for the average case where the LCE value is small, so our results in \pref{sec:results} does not include the variant of \fprintk\ with $O(k)$ average case query time.
\fi % article

%\begin{figure}[tp]
%    \begin{center}
%        \includegraphics[width=1\textwidth,page=1]{fingerprint-updown.pdf}
%    \end{center}
%    \caption{\label{fig:fingerprint-updown}Two ways of querying the fingerprint levels, one starting from level $\ell=k-1$ and one starting at level $\ell=0$.}
%\end{figure}

In step one of our query algorithm we perform one comparison at each level. We could instead do up to $O(n^{1/k})$ comparisons at each level without affecting asymptotic times.
\ifarticle
Our experiments have shown that always doing one comparison at each level is the best in practice.
\fi % article

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Preprocessing}

The tables of fingerprints use $O(k\cdot n)$ space. In the case with $k=\logceil$ levels, the data structure is the one generated by Karp-Miller-Rosenberg~\cite{karp-miller-rosenberg}. This data structure can be constructed in $O(n\log n)$ time. With $k<\logceil$ levels, KMR can be adapted, but it still uses $O(n\log n)$ preprocessing time.

We can preprocess the data structure in $O(\sortt + k\cdot n)$ time using the SA and LCP arrays. First create the SA and LCP arrays. Then preprocess each of the $k$ levels using the following steps:
\begin{samepage}
\begin{enumerate}
\item Loop through the $n$ substrings of length $t_\ell$ in lexicographically sorted order by looping through the elements of SA.
\item Assign an arbitrary fingerprint to the first substring.
\item If the current substring $s[\SA[i]\twodots\SA[i]+t_\ell-1]$ is equal to the substring examined in the previous iteration of the loop, give the current substring the same fingerprint as the previous substring, otherwise give the current substring a new unused fingerprint. The two substrings are equal when $\LCE[i] \geq t_\ell$.
\end{enumerate}

An example is shown in \pref{fig:fingerprint-preproc}.
\end{samepage}

\begin{figure}[tp]
    \begin{center}
        \includegraphics[width=0.2\textwidth,page=1]{../doc/fingerprint-preproc.pdf}
    \end{center}
    \caption{\label{fig:fingerprint-preproc}The first column lists all substrings of $s=abbababba$ with length $t_\ell = 3$. The second column lists fingerprints assigned to each substring. The third column lists the position of each substring in $s$.}
\end{figure}

\begin{lemma}
The preprocessing algorithm described above generates the data structure described in \pref{sec:fingerprint-ds}.
\end{lemma}

\begin{proof}
We always assign two different fingerprints whenever two substrings are different, because whenever we see two differing substrings, we change the fingerprint to a value not previously assigned to any substring.

We always assign the same fingerprint whenever two substrings are equal, because all substrings, which are equal, are grouped next to each other, when we loop through them in lexicographical order.
\end{proof}

\begin{samepage}
\begin{lemma}
The preprocessing algorithm described above takes $O(\sortt + k\cdot n)$ time.
\end{lemma}
\begin{proof}
We first construct the SA and LCP arrays, which takes $O(\sortt)$ time~\cite{sort-complexity}. We then preprocess each of the $k$ levels in $O(n)$ time, since we loop through $n$ substrings, and comparing neighboring substrings takes constant time when we use the LCE array. The total preprocessing time becomes $O(\sortt\\ + k\cdot n)$.
\end{proof}
\end{samepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Cache Optimization\label{sec:fingerprint-cache}}

%\paragraph{Horizontal, intra-level optimization}

The amount of I/O used by \fprintk\ is $O(k\cdot n^{1/k})$. However if we structure our tables of fingerprints differently, we can improve the number of I/O operations to $O(k(n^{1/k}/B+1))$ in the cache-oblivious model. Instead of storing the fingerprint of $s[i\twodots i+t_\ell-1]$ at $H_\ell[i]$, we can store it at $H_\ell[((i-1)\mod t_\ell)\cdot\lceil n/t_\ell\rceil+\lfloor (i-1)/t_\ell\rfloor+1]$. This will group all used fingerprints at level $\ell$ next to each other in memory, such that the amount of I/O at each level is reduced from $O(n^{1/k})$ to $O(n^{1/k}/B)$.

The size of each fingerprint table will grow from $|H_\ell| = n$ to $|H_\ell| = n+t_\ell$, because the rounding operations may introduce one-element gaps in the table after every $n/t_\ell$ elements. We achieve the greatest I/O improvement when $k$ is small. When $k=\logceil$, this cache optimization gives no difference in the amount of I/O.

\ifreport

\paragraph{Vertical, inter-level optimization}

For larger values of $k$, we can attempt to optimize memory access across different levels. When we move down through the levels for $k=\logceil$, we can continue in two directions after reading $H_\ell[i]$: $H_{\ell-1}[i]$ or $H_{\ell-1}[i+t_\ell]$. Neither of these positions are next to $H_\ell[i]$ in memory. We can optimize for the case where we reach $H_{\ell-1}[i]$ by flipping the direction in which we store the fingerprints, such that all fingerprints $H_\ell[i]$ for all $\ell$ and a specific $i$ are stored next to each other. This will not improve the asymptotic I/O bound, but could improve practical query times.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Space Usage}

\pref{tab:algorithms-space} shows the space requirement of each of our analyzed algorithms. The size of the input string of $n/4$ words is not included, and we ignore constant terms.

\begin{table}[tp]
\centering
\begin{minipage}{0.7\textwidth}
\centering
\begin{tabular}{l|l}
\hline\hline
Algorithm & Space \\ [0.5ex] \hline
\proc{DirectComp} & $0$ words \\ \hline
\proc{DirectLookup} & $2n^2$ words \footnote{The factor two comes from using shift instead of multiplication, which may double the size depending on $n$.} \\ \hline
\proc{DirectCompLookup} & $2n^2/t+n$ words \\ \hline
\proc{LcpRmq} & ca. $5.6n$ words \footnote{Space use for \proc{LcpRmq} is $2n + 2nb + (bs \cdot (bs+1) / 2 + 1)\cdot C_{bs} + (nb+1)\cdot \lfloor\log nb\rfloor \approx 5.6n$, where $bs = \lceil\log n / 4\rceil$, $nb = (n-1)/bs+1$ and $C_i$ is the $i$'t Catalan Number.} \\ \hline
\fprint[2] & $n$ words \\ \hline
\fprint[3] & $2n$ words \\ \hline
\fprint[\logceil] & $n\cdot\logceil$ words \\ \hline
\end{tabular}
\end{minipage}
\caption{Space requirements for the LCE algorithms, where $n$ is the number of characters, a character is one byte and a word is four bytes.}\label{tab:algorithms-space}
\end{table}

\fi % report

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental Results\label{sec:results}}

In this section we show results of actual performance measurements. The measurements were done on a Windows 23-bit machine with an Intel P8600 CPU (3 MB L2, 2.4 GHz) and 4 GB RAM. The code was compiled using GCC 4.5.0 with \texttt{-O3}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Tested Algorithms}

\ifarticle

We implemented different variants of the \fprintk\ algorithm in C++ and compared them with optimized versions of the \proc{DirectComp} and \proc{LcpRmq} algorithms. The algorithms we compared are the following:
\begin{description}
\item[\proc{DirectComp}] is the simple \proc{DirectComp} algorithm with no preprocessing and worst case $O(n)$ query time.
\item[\fprintk\textless$t_{k-1}$, ..., $t_1$\textgreater ac] is the \fprintk\ algorithm using $k$ levels, where $k$ is $2$, $3$ and $\logceil$. The numbers \textless$t_{k-1}$, ..., $t_1$\textgreater describe the exact size of fingerprinted substrings at each level.
\item[\proc{RMQ}\textless$n$, $1$\textgreater] is the \proc{LcpRmq} algorithm using constant time RMQ.
\end{description}

\fi % article

\ifreport

We implemented different variants of \proc{DirectComp}, \proc{LcpRmq}, \proc{DirectCompLookup} and \fprintk\ in C++ and compared them to each other. We have not implemented \proc{SuffixNca}, since others have shown that \proc{LcpRmq} is better than \proc{SuffixNca} in practice~\cite{ilie-navarro-tinta}. The algorithms we compared are the following:
\begin{description}
\item[\proc{DirectComp}] is the simple \proc{DirectComp} algorithm with no preprocessing and worst case $O(n)$ query time.
\item[\fprintk\textless$t_{k-1}$, ..., $t_1$\textgreater ac] is our new \fprintk\ algorithm using $k$ levels, where $k$ is $2$, $3$ and $\logceil$. The numbers \textless$t_{k-1}$, ..., $t_1$\textgreater\ describe the exact size of fingerprinted substrings at each level.
\item[\fprintk\textless$t_{k-1}$, ..., $t_1$\textgreater wc] is a variant of our new \fprintk\ algorithm, which starts at level $\ell=k-1$ instead of $\ell=0$, as discussed in \pref{sec:improved-avg}.
\item[\fprintk\textless$t_{k-1}$, ..., $t_1$\textgreater ac\textless$u_0$, ..., $u_{k-2}$\textgreater] is a variant of our new \fprintk\ algorithm, which stays at level $\ell$ until $v\geq u_\ell$, as discussed in \pref{sec:improved-avg}, instead of doing one comparison at each level in step one of the query in \pref{sec:fingerprint-query}.
\item[\proc{RMQ}\textless$n$, $1$\textgreater] is the \proc{LcpRmq} algorithm using constant time RMQ.
\item[\proc{RMQ}\textless$p$, $q$\textgreater$_{\textrm{virtual}}$] is the \proc{LcpRmq} algorithm using different RMQ algorithms. They all contain an additional virtual method call to make implementation simpler, but their results cannot be directly compared to other algorithms, which do not have this extra virtual method call.
\item[\proc{DirectComp}\textless$c$\textgreater\proc{RMQ}\textless$n$, $1$\textgreater] is the \proc{DirectComp+LcpRmq} algorithm, where the cutoff from \proc{DirectComp} to \proc{LcpRmq} is after $c$ iterations.
\item[\fprintk\textless$t_{k-1}$, ..., $t_1$\textgreater ac$_{\textrm{cache}-\textit{dir}-\textit{op}}$] is a cache optimized variant of our \fprintk\ algorithm, where \textit{dir} describes which of our two cache optimized variants is used, and \textit{op} describes which of the shift or multiplication arithmetic operations it uses.
\end{description}

\fi % report

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Test Inputs and Setup}

We have tested the algorithms on different kinds of strings:
\begin{description}
\item[Average case strings] These strings have many small LCE values, such that the average LCE value over all $n^2$ query pairs is less than one. We use results on these strings as an indication average case query times over all input pairs $(i,j)$ in cases where most or all LCE values are small on expected input strings. We construct these strings by choosing each character uniformly at random from an alphabet of size 10
\item[Worst case strings] These strings have many large LCE values, such that the average LCE value over all $n^2$ query pairs is $n/2$. We use results on these strings as an indication of worst case query times, since the query times for all tested algorithms are asymptotically at their worst when the LCE value is large. We construct these strings with an alphabet size of one.
\item[Medium LCE value strings] These strings have an average LCE value over all $n^2$ query pairs of $n/2r$, where $r=0.73n^{0.42}$. We use results on these strings as an indication of query times somewhere between the average case and worst case. We construct these strings by repeating a substring of $r$ characters, where each character is unique.
\end{description}

For each kind of strings, we tested the algorithms using the following pattern:
\begin{enumerate}
\item Generate a string of length $n$ and generate a million random pairs $(i, j)$, where each of $i$ and $j$ is chosen between $1$ and $n$ uniformly at random.
\item For each tested algorithm, preprocess the given string, and run a query for each of the million pairs. Measure the time it takes to run all the queries combined.
\item Double the value of $n$ and repeat from step one.
\end{enumerate}

\ifreport

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\proc{LcpRmq} Results\label{sec:test-rmq}}

We want to compare our new algorithms against the algorithm which is the best in practice of the asymptotically best $O(1)$ algorithms. The \proc{LcpRmq} algorithm is better than \proc{SuffixNca}~\cite{ilie-navarro-tinta}, so we only look at \proc{LcpRmq}. We have looked at a number of RMQ algorithms to find out which works best for the LCE problem. The results are shown in \pref{fig:plot-rmq}. \RMQq\ is the best in practice, and we therefore use that when we test against other LCE algorithms. This result is different from that of Ilie~et~al.~\cite{ilie-navarro-tinta}, where \RMQpq{n}{4\log n} is the best. This could be due to hardware or compiler differences.

The performance of the implementations shown in \pref{fig:plot-rmq} cannot be compared with those shown on other figures, because they all contain an additional, slow virtual method call, which gives them an unfair disadvantage. We have optimized the best variant of \proc{LcpRmq} to avoid the virtual method call, when we compare it against other algorithms. We compare this optimized version to \proc{DirectComp} in \pref{fig:plot-rmq-directcomp}, which shows that \proc{DirectComp} is significantly better in average case, though both have constant asymptotic average case query times.

The RMQ algorithms show a significant jump in query times around $n=1.000.000$ on the plot with average case strings, but not on the plot with worst case strings. We have run the tests in Cachegrind, and found that the number of instructions executed and the number of data reads and writes are exactly the same for both average case strings and worst case strings. The cache miss rate for \RMQq\ on average case strings is 14\% and 9\% for the L1 and L2 caches, and on worst case strings the miss rate is 17\% and 13\%, which is the opposite of what could explain the jump we see in the plot.

% size = 3,5*2^30 = 3758096384
% assoc = (3,5*2^30)/(4*2^10) = 917504
% line = 4*2^10 = 4096
% --LL=3758096384,917504,4096

% size = 1*2^30 = 1073741824
% assoc = (1*2^30)/(4*2^10) = 262144
% line = 4*2^10 = 4096
% --LL=1073741824,262144,4096

\begin{figure}[p]
    \begin{center}
        \includegraphics[width=0.49\textwidth,type=pdf,ext=.pdf,read=.pdf]{../src/results/length-rmq-rand10.plt}
        \includegraphics[width=0.49\textwidth,type=pdf,ext=.pdf,read=.pdf]{../src/results/length-rmq-alla.plt}
    \end{center}
    \caption{\label{fig:plot-rmq}Query times of \proc{LcpRmq} using different variants of RMQ.}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{\proc{DirectComp+LcpRmq} Results}

\pref{fig:plot-rmq-directcomp} also shows \proc{DirectComp+LcpRmq}, where we first try \proc{DirectComp}, and if we have not found the LCE value after one iteration, we use \proc{LcpRmq} instead. We see that \proc{DirectComp+LcpRmq} is worse than \proc{DirectComp} on average case strings. We could increase the cutoff from one iteration to something bigger, which might improve average case performance. That would however increase worst case time. None of the two plots we measure represents a good worst case for \proc{DirectComp+LcpRmq} compared to the other algorithms we test. The worst case for \proc{DirectComp+LcpRmq} relative to the other algorithms would be when the average LCE value is slightly larger than the cutoff, such that \proc{DirectComp+LcpRmq} reaches \proc{LcpRmq} in most queries, while other algorithms still work on LCE values as small as possible. As described in \pref{sec:dc-lcprmq}, \proc{DirectComp+LcpRmq} also uses a lot of space.

\begin{figure}[p]
    \begin{center}
        \includegraphics[width=0.49\textwidth,type=pdf,ext=.pdf,read=.pdf]{../src/results/length-rmq-directcomp-rand10.plt}
        \includegraphics[width=0.49\textwidth,type=pdf,ext=.pdf,read=.pdf]{../src/results/length-rmq-directcomp-alla.plt}
    \end{center}
    \caption{\label{fig:plot-rmq-directcomp}Query times of constant time \proc{LcpRmq}, \proc{DirectComp} and \proc{DirectComp+LcpRmq}.}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\proc{DirectCompLookup} Results}

\begin{figure}[p]
    \begin{center}
        \includegraphics[width=0.49\textwidth,type=pdf,ext=.pdf,read=.pdf]{../src/results/length-complookup-rand10.plt}
        \includegraphics[width=0.49\textwidth,type=pdf,ext=.pdf,read=.pdf]{../src/results/length-complookup-alla.plt}
    \end{center}
    \caption{\label{fig:plot-complookup}Query times of our new DirectCompLookup algorithm versus the existing \proc{DirectComp} and \proc{LcpRmq} algorithms.}
\end{figure}

\pref{fig:plot-complookup} shows \proc{DirectCompLookup} with different time/space-tradeoffs together with \proc{LcpRmq}. In average case, \proc{DirectCompLookup} is slower than \proc{DirectComp}, most likely because it needs to do an extra check to find out if it has reached a position for which a result is stored. For short strings, \proc{DirectLookup} is faster than \proc{DirectCompLookup}, since it does not need to check if the value it needs is stored. However, it quickly grows to be as slow as \proc{LcpRmq}, while \proc{DirectCompLookup} does not experience this slowdown. We explain this with that \proc{DirectLookup} reads from its $O(n^2)$ table of results in every query, whereas \proc{DirectCompLookup} only reads from the $O(n)$ string in average case, and thus has a significantly smaller working set.

In the worst case, the \proc{DirectCompLookup} variants which use more space are faster in practice, just like in theory. If we are willing to use $O(n\log n)$ space, \proc{DirectCompLookup}\textless$n/\log n$\textgreater\ will only halve the worst case query time compared to \proc{DirectComp}. This makes \proc{DirectCompLookup} better than \proc{DirectComp}, but still not a very good candidate when worst case query time is important.

\proc{DirectCompLookup} uses $O(n^2)$ preprocessing time no matter what parameter $t$ we choose, which is significantly slower than any of the other algorithms we have tested, and it is also the reason why we have only tested this algorithm up until $n=100.000$. The only way we have found to improve this preprocessing time is to use one of the other algorithms we have covered to preprocess the data structure of \proc{DirectCompLookup}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\fprintk\ Results}

\fi % report

\ifarticle
\subsection{Results}
\fi % article

\begin{figure}[tp]
    \begin{center}
        \includegraphics[width=0.7\textwidth,type=pdf,ext=.pdf,read=.pdf]{../src/results/length-article-rand10.plt}
        \includegraphics[width=0.7\textwidth,type=pdf,ext=.pdf,read=.pdf]{../src/results/length-article-alla.plt}
        \includegraphics[width=0.7\textwidth,type=pdf,ext=.pdf,read=.pdf]{../src/results/length-article-repeat-pow.plt}
    \end{center}
    \caption{\label{fig:test-fingerprint}Comparison of our new \fprintk\ algorithm for $k=2$, $k=3$ and $k=\logceil$ versus the existing \proc{DirectComp} and \proc{LcpRmq} algorithms.}
\end{figure}

\begin{table}[tp]
\centering
\begin{tabular}{l|r|r|r|r|r|r|r}
\hline\hline
% File & \proc{DirectComp} & \fprint[2]\textless$\sqrt n$\textgreater ac & \fprint[3]\textless$n^{2/3}$, $n^{1/3}$\textgreater ac & \fprint[\log n]ac & RMQ<n;1> \\ [0.5ex] \hline
File & $n$ & $\sigma$ & DC & FP$_2$ & FP$_3$ & FP$_{\log n}$ & RMQ \\ [0.5ex] \hline
book1 & $0.7\cdot 2^{20}$ & 82 & 8.1 & 11.4 & 10.6 & 12.0 & 218.0 \\ \hline
kennedy.xls & $1.0\cdot 2^{20}$ & 256 & 11.9 & 16.0 & 16.1 & 18.6 & 114.4 \\ \hline
E.coli & $4.4\cdot 2^{20}$& 4 & 12.7 & 16.5 & 16.6 & 19.2 & 320.0 \\ \hline
bible.txt & $3.9\cdot 2^{20}$ & 63 & 8.5 & 11.3 & 10.5 & 12.6 & 284.0 \\ \hline
world192.txt & $2.3\cdot 2^{20}$ & 93 & 7.9 & 10.5 & 9.8 & 12.7 & 291.7 \\ \hline
\end{tabular}
\caption{Query times in nano seconds for \proc{DirectComp} (DC), \fprintk\ (FP$_k$) and \proc{LcpRmq} (RMQ) on the five largest files from the Canterbury corpus.}\label{tab:text-fprint-files}
\end{table}

\pref{fig:test-fingerprint} shows our experimental results on average case strings with a small average LCE value, worst case strings with a large average LCE value, and strings with a medium average LCE value.

On average case strings, our new \fprintk\ algorithm is approximately 20\% slower than \proc{DirectComp}, and it is between than 5 and 25 times faster than \proc{LcpRmq}. We see the same results on some real world strings in \pref{tab:text-fprint-files}.

On worst case strings, the \fprintk\ algorithms are significantly better than \proc{DirectComp} and somewhat worse than \proc{LcpRmq}. Up until $n=30.000$ the three measured \fprintk\ algorithms have nearly the same query times. Of the \fprintk\ algorithms, the $k=2$ variant has a slight advantage for small strings of length less than around $2.000$. For longer strings the $k=3$ variant performs the best up to strings of length $250.000$, at which point the $k=\logceil$ variant becomes the best. This indicates that for shorter strings, using fewer levels is better, and when the input size increases, the \fprintk\ variants with better asymptotic query times have better worst case times in practice.

On strings with medium average LCE values, we see that our \fprintk\ algorithms are faster than both \proc{DirectComp} and \proc{LcpRmq}

We conclude that our new \fprintk\ algorithm achieves a tradeoff between worst case times and average case times, which is better than the existing best \proc{DirectComp} and \proc{LcpRmq} algorithms, yet it is not strictly better than the existing algorithms on all inputs. \fprintk\ is therefore a good choice in cases where both average case and worst case performance is important.

%  \fprint[3]\ also shows such a jump, but for greater input sizes, as it uses less memory.

\ifarticle

\proc{LcpRmq} shows a significant jump in query times around $n=1.000.000$ on the plot with average case strings, but not on the plot with worst case strings. We have run the tests in Cachegrind, and found that the number of instructions executed and the number of data reads and writes are exactly the same for both average case strings and worst case strings. The cache miss rate for average case strings is 14\% and 9\% for the L1 and L2 caches, and for worst case strings the miss rate is 17\% and 13\%, which is the opposite of what could explain the jump we see in the plot.

\fi % article

% \fixme{Results: Investigate if we can explain the jump in Fingerprint-log}
% \paragraph{\fprint[\log n] cache limit.}
% \proc{LcpRmq} uses more space than all of \proc{DirectComp}, \fprint[2] and \fprint[3], so it is expected that it reaches the cache limit before these. However \fprint[\log n] uses more space than \proc{LcpRmq} at the input sizes we measure, since \proc{LcpRmq} uses the same amount of memory as \fprint[6], and $\lceil\log(10.000.000)\rceil = 24$. However, the \fprintk\ algorithms only access the lowest levels of their data structure whenever the LCE values are small, and there is therefore no visible jump due to cache limits for \fprint[\log n] on the plot with average case strings. On the plot with worst case strings however, we see a jump in \fprint[\log n] between $n=20.000$ and $n=50.000$, since it uses all its $\logceil$ levels when the LCE values are large.

\ifreport

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Optimal Value of $t_\ell$}

\pref{fig:plot-fingerprint-tval} shows \fprint[2] for different values of $t_1$. On average case strings the value of $t_1$ makes no difference, and on worst case strings a value of $t_1$ between $2\sqrt n$ and $4\sqrt n$ is the best. The improvement relative to $\sqrt n$ is however insignificant, so we have chosen to use $t_1=\sqrt n$.

Our results indicate that reading values from $H_0$ is faster than reading values from $H_1$. Differences between these two tables include that elements of $H_0 = s$ use one byte, whereas elements of $H_1$ use four bytes, and that elements we read from $H_0$ are located next to each other in memory, whereas elements we read from $H_1$ are spaced $t_1$ elements apart in memory.

\begin{figure}[p]
    \begin{center}
        \includegraphics[width=0.49\textwidth,type=pdf,ext=.pdf,read=.pdf]{../src/results/length-fingerprint-2l-rand10.plt}
        \includegraphics[width=0.49\textwidth,type=pdf,ext=.pdf,read=.pdf]{../src/results/length-fingerprint-2l-alla.plt}
    \end{center}
    \caption{\label{fig:plot-fingerprint-tval}Query times of \fprint[2] for different values of $t_1$ versus query times for \proc{DirectComp}.}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Average Case Optimization\label{sec:test-average}}

In \pref{sec:improved-avg} we discussed how our \fprintk\ algorithm is optimized for the average case to get $O(1)$ instead of $O(k)$ average case query time, while it keeps the same asymptotic worst case query time. But in practice the worst case query time is slightly affected.

\begin{figure}[p]
    \begin{center}
        \includegraphics[width=0.49\textwidth,type=pdf,ext=.pdf,read=.pdf]{../src/results/length-fingerprint-avglog-rand10.plt}
        \includegraphics[width=0.49\textwidth,type=pdf,ext=.pdf,read=.pdf]{../src/results/length-fingerprint-avglog-alla.plt}
    \end{center}
    \caption{\label{fig:plot-fingerprint-avglog}Query times of \fprint[\logceil] with $O(1)$ versus $O(k)$ average case query time.}
\end{figure}

In \pref{fig:plot-fingerprint-avglog} we see that on worst case strings, the query time of \fprint[\logceil] doubles when we use the average case optimization. On average case strings, the query time of \fprint[\logceil] is up to 40 times faster when using average case optimization. All four cases in \pref{fig:plot-fingerprint-avglog} show their expected $O(1)$ and $O(\log n)$ query times. In \pref{fig:plot-fingerprint-avg3l}, we see the same results for \fprint[3], as we saw for \fprint[\logceil], but at a smaller scale. The negative effect of doing average case optimization is barely measurable on worst case strings, while the improvement on average case strings is up to 40\%.

\begin{figure}[p]
    \begin{center}
        \includegraphics[width=0.49\textwidth,type=pdf,ext=.pdf,read=.pdf]{../src/results/length-fingerprint-avg3l-rand10.plt}
        \includegraphics[width=0.49\textwidth,type=pdf,ext=.pdf,read=.pdf]{../src/results/length-fingerprint-avg3l-alla.plt}
    \end{center}
    \caption{\label{fig:plot-fingerprint-avg3l}Query times of \fprint[3] with $O(1)$ versus $O(k)$ average case query time.}
\end{figure}

In \pref{fig:plot-fingerprint-avg3l} we see that doing one comparison at each level on the way up in \fprint[3] is better than doing $n^{1/k}$ comparisons. This is most visible on worst case strings, where the algorithm doing one comparison at each level uses less time to reach level $\ell=k-1$, which it will reach anyway. On average case strings, the code specialized in handling only one comparison at each level is simpler.

We conclude that using average case optimization yields a query time improvement on average case strings, which is significantly greater than the query time increase on worst case strings, and that \fprintk\ therefore should use the average case optimization.

\fi % report

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Cache Optimization}

\begin{figure}[tp]
    \begin{center}
        \includegraphics[width=0.49\textwidth,type=pdf,ext=.pdf,read=.pdf]{../src/results/length-fingerprint-cache-rand10.plt}
        \includegraphics[width=0.49\textwidth,type=pdf,ext=.pdf,read=.pdf]{../src/results/length-fingerprint-cache-alla.plt}
        \includegraphics[width=0.49\textwidth,type=pdf,ext=.pdf,read=.pdf]{../src/results/length-fingerprint-cache-repeat-pow.plt}
        \includegraphics[width=0.49\textwidth,type=pdf,ext=.pdf,read=.pdf]{../src/results/length-fingerprint-cache-rand2.plt}
    \end{center}
    \caption{\label{fig:plot-fingerprint-cache-horiz}Query times of \fprint[2] with and without intra-level cache optimization.}
\end{figure}

\ifreport
\begin{figure}[p]
    \begin{center}
        \includegraphics[width=0.49\textwidth,type=pdf,ext=.pdf,read=.pdf]{../src/results/length-fingerprint-log-cache-rand10.plt}
        \includegraphics[width=0.49\textwidth,type=pdf,ext=.pdf,read=.pdf]{../src/results/length-fingerprint-log-cache-alla.plt}
    \end{center}
    \caption{\label{fig:plot-fingerprint-cache-vert}Query times of \fprint[2] with and without inter-level cache optimization.}
\end{figure}
\fi % report

%\paragraph{Horizontal, intra-level optimization}

\pref{fig:plot-fingerprint-cache-horiz} shows results of the cache optimization described in \pref{sec:fingerprint-cache}. We have implemented two cache optimized variants. One as described in \pref{sec:fingerprint-cache}, and one where multiplication and division is replaced with shift operations. To use shift operations, $t_\ell$ and $\lceil n/t_\ell\rceil$ must both be powers of two. This may double the size of the used address space.

On average case strings the cache optimization does not change the query times, while on worst case strings and strings with medium size LCE values, cache optimization gives a noticeable improvement for large inputs. The cache optimized \fprint[3] variant with shift operations shows an increase in query times for large inputs, which we cannot explain.

The last plot on \pref{fig:plot-fingerprint-cache-horiz} shows a variant of average case where the alphabet size is changed to two. This plot attempts to show the worst case for cache optimized \fprint[3]. LCE values in this plot are large enough to ensure that $H_1$ is used often, which should make the extra complexity of calculating indexes into $H_1$ visible. At the same time the LCE values are small enough to ensure, that the cache optimization has no effect. In this plot we see that the cache optimized variant of \fprint[3] has only slightly worse query time compared to the variant, which is not cache optimized.

We conclude that this cache optimization does not visibly affect average case query times, while it improves worst case query times. However, due to late timing of this discovery, the cache optimized variant of \fprintk\ is not the main focus of our analysis.

\ifreport

\paragraph{Vertical, inter-level optimization}

\pref{fig:plot-fingerprint-cache-vert} shows results of our inter-level cache optimization. We see no improvement on worst case strings, and we see increased query times on average case strings. We conclude that this attempt at cache optimization does not improve practical query times, which could be expected, since it does not improve the asymptotic I/O bound either.

\fi % report

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}

We have presented the \fprintk\ algorithm, where $k$ is a parameter $1 \leq k \leq \logceil$, which for a string $s$ of length $n$ and alphabet size $\sigma$, can solve the LCE problem in $O(k\cdot n^{1/k})$ worst case query time and $O(1)$ average case query time using $O(k\cdot n)$ space and $O(\sortt + k\cdot n)$ preprocessing time.

\ifreport
Of the existing best \proc{DirectComp} and \proc{LcpRmq} algorithms, we have seen that \proc{DirectComp} is best on average case strings, while \proc{LcpRmq} is best on worst case strings.

We have found that our new \proc{DirectCompLookup} algorithm can improve the worst case query time performance of \proc{DirectComp}, while it does not degrade the average case query time to the level of \proc{LcpRmq}. However it is not as interesting as our new \fprintk\ algorithm, because it uses too much space.
\fi % report

The \fprintk\ algorithm is able to achieve a balance between practical worst case and average case query times. It has almost as good average case query times as \proc{DirectComp}, its worst case query times are significantly better than those of \proc{DirectComp}, and we have found cases between average and worst case where \fprintk\ is better than both \proc{DirectComp} and \proc{LcpRmq}. \fprintk\ gives a good time space tradeoff, and it uses less space than \proc{LcpRmq} when $k$ is small. %The performance of \fprintk\ can be tweaked on many parameters, and we have found that optimizing for average case queries and for memory caches improve practical query time performance.

\ifreport

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Test Code}

To test practical performance, we have implemented the algorithms in C++. The implementation is based on that of Ilie~et~al.~\cite{ilie-navarro-tinta}, which we have heavily modified. We have added a working build system, made it compile on a 32-bit Windows architecture, and generalized it to allow more algorithms and tests to be implemented. \pref{tab:test-variables} summarizes the structure of the code and the parameters we can vary.

\begin{table}[tp]
\centering
\begin{tabular}{l l l}
Variable & Example & Component \\ \hline
alphabet size & $\sigma = 10$ & string \\
character pattern & uniformly at random & string \\ \hline
algorithm & \fprintk & algorithm \\
alg. parameters & $t_1 = 2\sqrt n$ & algorithm \\ \hline
string length & exponentially increasing from $1$ to $100.000$ & test \\
query pattern & uniformly at random & test \\ \hline
\end{tabular}
\caption{List of performance variables tested.}\label{tab:test-variables}
\end{table}

In our implementation, we have made the following assumptions about the LCE problem:
\begin{itemize}
\item A character in a string is always eight bits.
\item A string always ends with a special null character, which does not occur anywhere else in the string. This does not affect our results, since we do not measure practical preprocessing time. If an algorithm has faster query times when the string ends in a special character, it could just add that character to the end of the string in its preprocessing.
\item We make no assumptions on the order of $i$ and $j$. In some applications of the LCE problem, for example approximate string searching~\cite{approx-search}, we know that $i<j$, and some algorithms might be able to take advantage of this knowledge to get faster query times. We do not measure this case.
\item Whenever we use random characters or query pairs, we use the \texttt{rand()} function from C. This gives repeatable pseudo-random numbers, which makes it easier for us to compare test runs.
\end{itemize}

% TODO \fixme{Results: Test Code: tell about c++ bug hunting in tinta, cachegrind code "Cache set count is not a power of two."}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Query time by LCE value}

\begin{figure}[tp]
    \begin{center}
        %\includegraphics[width=0.49\textwidth,type=pdf,ext=.pdf,read=.pdf]{../src/results/value-article-rand4.plt}
        \includegraphics[width=0.49\textwidth,type=pdf,ext=.pdf,read=.pdf]{../src/results/value-article-alla.plt}
        \includegraphics[width=0.49\textwidth,type=pdf,ext=.pdf,read=.pdf]{../src/results/value-article-repeat10-10.plt}
    \end{center}
    \caption{\label{fig:plot-by-value}Query times of some of our tested algorithms as a function of the returned LCE value.}
\end{figure}

As an alternative way of testing our algorithms, we have looked at how long a query takes in relation to the LCE value it returns, since most of our algorithms have query times, which depend on the returned LCE value. \pref{fig:plot-by-value} shows query times for a fixed string of 1000 characters. We can see how the query times of \fprint[2] and \fprint[3] increases as the LCE value increases, and drops when the LCE value becomes long enough to make one more step at the higher level with fingerprints of longer substrings. We also see that the query time of \fprint[\log n] increases whenever the LCE value becomes long enough to reach a higher level of the data structure.

The plots in \pref{fig:plot-by-value} contains a lot of noise compared to our other plots. The number of measurements we make on these plots is many times bigger than the number of measurements we make on other plots. Therefore each measurement is an average over only 10000 queries instead of a million queries, which gives less accurate results.

We need to measure a sum of many queries to get an accurate timing, since each individual query is too fast to measure. While measuring a sequence of randomly generated queries is a very artificial test, which is unlikely to occur in practice, we think that it is even more artificial to measure a sequence of queries for which we already know that the LCE value is the same each time. On our worst case string $s=aaaaaaa...$, the queries we measure in each point in \pref{fig:plot-by-value} have the pattern $\max(i,j)=v$, which may give very different performance characteristics compared to random queries due to cache effects. We cannot meaningfully measure performance by LCE value on average case strings, as these strings do not contain large LCE values. The second plot in \pref{fig:plot-by-value} is on a string, which attempts to avoid a strong pattern like $\max(i,j)=v$ while it still has large LCE values.

Because of the amount of noise in the plots and the strong pattern of $\max(i,j)=v$, we have not focused on this way of measuring query time performance in our analysis.

\begin{comment}
\begin{algorithm}
\caption{Algorithm for plotting query time as a function of string length\label{alg:graph-length}}
\begin{codebox}
\li \For $\id{type} \in \{\textrm{string of random characters with}~\sigma=10,~\textrm{string with}~\sigma = 1\}$
    \Indentmore
\li     \For $n = 1$ \To $1e7$ \Comment Double $n$ at each iteration
        \Indentmore
\li         $s = \proc{MakeString}_{\id{type}}(n)$
\li         $\id{data} =$ a sequence of a million pairs $(i,j)$ where
            \Indentmore
\zi             $i\in[1\twodots n]$ and $j\in[1\twodots n]$ are chosen
\zi             uniformly at random.
            \End
\li         \For each $\id{alg} \in \id{Algorithms}$
            \Indentmore
\li             $\id{alg}.\proc{preprocess}(s)$
\li             start timing
\li             \For each $(i,j) \in \id{data}$
                \Indentmore
\li                 $\id{alg}.\proc{query}(i,j)$
                \End
\li             end timing
\li             \proc{Plot}$_{\id{type}}$($alg$, $n$, timing)
\li             $\id{alg}.\proc{cleanup}()$
            \End
        \End
    \End
\end{codebox}
\end{algorithm}

\begin{algorithm}
\caption{Algorithm for plotting query time as a function of LCE value\label{alg:graph-value}}
\begin{codebox}
\li $s = \proc{MakeString}(1000)$
\li \id{data} = an array where $\id{data}[v] \subseteq \{(i,j)~|~\LCE(i,j)=v\} \land |\id{data}[v]| =10.000$
\li \For each $\id{alg} \in \id{Algorithms}$
    \Indentmore
\li     $\id{alg}.\proc{preprocess}(s)$
\li     \For $v = 0$ \To $n$
        \Indentmore
\li         \Repeat 3 times
% \li     $A =$ generate 1000000 random pairs $(i,j)$ where $i<j$
\li             start timing
\li             \For each $(i,j) \in data[v]$
                \Indentmore
\li                 $\id{alg}.\proc{query}(i,j)$
                \End
\li             end timing
            \End
\li         \proc{Plot}($alg$, $v$, median of the three timings)
        \End
\li     $\id{alg}.\proc{cleanup}()$
    \End
\end{codebox}
\end{algorithm}

Where $\proc{Plot}(f,x,y)$ adds point $(x,y)$ to series $f$.
\end{comment}

\fi % report

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{thebibliography}{9}

\bibitem{ilie-navarro-tinta} Lucian Ilie, Gonzalo Navarro, and Liviu Tinta. The longest common extension problem revisited and applications to approximate string searching. Journal of Discrete Algorithms, Volume 8, Issue 4, December 2010, pages 418-428.

% \bibitem{linear-lcp} Toru Kasai, Gunho Lee, Hiroki Arimura, Setsuo Arikawa, and Kunsoo Park. Linear-Time Longest-Common-Prefix Computation in Suffix Arrays and Its Applications. CPM 2001, LNCS 2089, 2001, pages 181-192.

\bibitem{sort-complexity} Martin Farach-Colton, Paolo Ferragina, and S. Muthukrishnan. On the sorting-complexity of suffix tree construction. J. ACM Vol. 47, No. 6, November 2000, pages 987-1011.

\bibitem{karp-miller-rosenberg} Richard M. Karp, Raymond E. Miller, and Arnold L. Rosenberg. 1972. Rapid identification of repeated patterns in strings, trees and arrays. In Proceedings of the fourth annual ACM symposium on Theory of computing (STOC '72). ACM, New York, NY, USA, 125-136.

\bibitem{nca} D. Harel, R. E. Tarjan. Fast Algorithms for Finding Nearest Common Ancestors. SIAM J. Comput., 1984.
% S. Alstrup, C. Gavoille, H. Kaplan, and T. Rauhe. Nearest Common Ancestors: A Survey and a New Algorithm for a Distributed Environment. Theory of Comput. Sys., 2004.

\bibitem{jf-rmq} Johannes Fischer, and Volker Heun. Theoretical and Practical Improvements on the RMQ-Problem, with Applications to LCA and LCE. Proceedings of the 17th Annual Symposium on Combinatorial Pattern Matching (CPM'06), Lecture Notes in Computer Science 4009, 36-48, Springer-Verlag, 2006.

%\bibitem{predecessor} Dan E. Willard. Log-logarithmic worst-case range queries are possible in space $\Theta$(N). Information Processing Letters, Volume 17, Issue 2, 24 August 1983, pages 81-84.

%\bibitem{level-ancestor} Michael A. Bendera, Martín Farach-Colton. The Level Ancestor Problem simplified. Theoretical Computer Science 321, 2004, pages 5-12.

\bibitem{approx-search} Gad M. Landau and Uzi Vishkin. Introducing efficient parallelism into approximate string matching and a new serial algorithm. 18th ACM STOC, pages 220–230, 1986.

\end{thebibliography}

\ifreport

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section*{THE END}

\begin{comment}
\subsection*{Tinta tests}
The two tests of LCE query performance used in the Tinta paper are shown in \pref{alg:tinta-testlceonebyone} and \pref{alg:tinta-randomtestlce}.

% http://www.cs.dartmouth.edu/~thc/clrscode/clrscode.sty

\begin{algorithm}
\caption{Test algorithm for \texttt{test\_lce\_one\_by\_one}\label{alg:tinta-testlceonebyone}}
\begin{codebox}
\li \For each $LCE \in Algorithms$
    \Indentmore
\li     \For $k = 2$ \To $n$
        \Indentmore
\li         $A =\{(i,j) ~|~ j-i=k\}$
\li         \Repeat 3 times
\li             shuffle $A$
\li             start timing
\li             \For each $(i,j) \in A$
                \Indentmore
\li                 $LCE(i,j)$
                \End
\li             end timing
            \End
\li         $avg =$ average of the 3 timings
\li         log(each of the 3 timings)
\li         print($avg/length(A)$)
        \End
    \End
\end{codebox}
\end{algorithm}

\begin{algorithm}
\caption{Test algorithm for \texttt{random\_test\_lce}\label{alg:tinta-randomtestlce}}
\begin{codebox}
\li \Repeat 3 times
\li     $A =$ generate 1000000 random pairs $(i,j)$ where $i<j$
\li     \For each $\proc{LCE} \in \id{Algorithms}$
        \Indentmore
\li         start timing
\li         \For each $(i,j) \in A$
            \Indentmore
\li             $LCE(i,j)$
            \End
\li         end timing
        \End
    \End
\li \For each $LCE \in Algorithms$
    \Indentmore
\li     log(each of the 3 timings)
    \End
\li \For each $LCE \in Algorithms$
    \Indentmore
\li     print($($ average of the 3 timings $)/length(A)$)
    \End
\end{codebox}
\end{algorithm}
\end{comment}

\ifarticle
\fxfatal{Opskriv funktioner for plads.}
\fi % article

\begin{comment}
\subsection*{Dropped}
\begin{enumerate}
\item DOC: Test on another machine.
\item CODE: Improve cache-vertical and make shift variant.
\item CODE: Non-random strings? Non-random queries? Different t-values for Hash-3L?
\end{enumerate}

\listoffixmes

\section{What is the time for KMR?}
\subsection{Original KMR}
"Rapid identification of repeated patterns in strings, trees and arrays"
says $O(n\cdot(\log n)^2)$.

http://portal.acm.org.globalproxy.cvt.dk/citation.cfm?id=804905

\subsection{RMR in $O(n \log n)$}
\begin{itemize}
\item
"A Theoretical and Experimental Study on the Construction of Suffix Arrays in External Memory"
page 10, 3.3 The Doubling Algorithm.

http://www.springerlink.com.globalproxy.cvt.dk/content/h1tejj3bamjj5qrg/
\item
"Compressing and indexing labeled trees, with applications"

http://portal.acm.org.globalproxy.cvt.dk/citation.cfm?id=1613680
\end{itemize}

KMR uses $O(sort(n,n^2)\cdot\log n) = O(n\log n)$ time.

At a given level:
\begin{enumerate}
\item create 2-touples of fingerprints from the previous level.
\item sort these touples lexicographically. (radix sort takes $O(kn)$ for $\sigma=n^k$, here $\sigma=n^2$)
\item scan through the sorted touples to assign new fingerprints.
\end{enumerate}
\end{comment}

\fi % report

\end{document}
